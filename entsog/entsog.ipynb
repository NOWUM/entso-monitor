{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTSO-G Daten\n",
    "\n",
    "Die Daten der European Network of Transmission System Operators for Gas (ENTSO-G) werden auf dem Transparenzportal veröffentlicht.\n",
    "\n",
    "Über eine API lassen sich die Daten, sowie Historiendaten abfragen.\n",
    "\n",
    "Das Projekt unterteilt sich zunächst in 3 Teile:\n",
    "\n",
    "1. Daten abfragen und speichern\n",
    "2. Daten aufbereiten\n",
    "3. Daten visualisieren\n",
    "\n",
    "Im Gegensatz zu den ENTSO-E Daten gibt es hierbei noch keinerlei Web-Frontends, welche die Daten visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Definitionen\n",
    "\n",
    "* Bidding zones sind die Marktgebiete. Um zwischen Marktgebieten Gas zu transferieren müssen die Gas-Pipelines gebucht werden. \n",
    "* Jedes Land hat sein eigenes Marktgebiet (mit Ausnahmen, bspw hat DE bis 2021 noch 2 [NCG und GASPOOL])\n",
    "* In einem Marktgebiet gibt es verschiedene Operatoren, welche das Gasnetz betreiben\n",
    "* Es gibt L-Gas und H-Gas, welche unterschiedlich gebucht und transferiert werden\n",
    "* Jede Pipeline hat einen Entry und einen Exit. Gas fließt immer in eine Richtung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So gibt es bspw. die Pipeline \"Nord Stream\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenbeschaffung\n",
    "\n",
    "Die Daten können über die API in CSV, JSON oder XML runtergeladen werden.\n",
    "\n",
    "Tabellarische Daten lassen sich davon am effizientesten als CSV darstellen, weshalb dieses Format gewählt wurde.\n",
    "\n",
    "Diese werden anschließend in eine Parquet-Datei gespeichert.\n",
    "Darüber können wir nun über die Historien-Daten zugreifen.\n",
    "\n",
    "Die Daten der ENTSO-G stehen in diesem Format über die API seit 2017 zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggData = pd.read_csv('https://transparency.entsog.eu/api/v1/AggregatedData.csv?limit=1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# suddenly stopped working at the beginning of 2021\n",
    "#aggData = pd.read_csv('https://transparency.entsog.eu/api/v1/AggregatedData.csv?limit=1000')\n",
    "interconn = pd.read_csv('https://transparency.entsog.eu/api/v1/Interconnections.csv?limit=-1')\n",
    "# the connectionpoints contain invalid data, as each line ends with an extra seperator\n",
    "conn = pd.read_csv('https://transparency.entsog.eu/api/v1/connectionpoints.csv?limit=-1',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "csv = requests.get('https://transparency.entsog.eu/api/v1/connectionpoints.csv?limit=-1')\n",
    "cleaned = csv.text.replace('commercialType,importFromCountryKey','installation,commercialType,importFromCountryKey')\n",
    "from io import StringIO\n",
    "fileLike = StringIO(cleaned)\n",
    "conn = pd.read_csv(fileLike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsn = requests.get('https://transparency.entsog.eu/api/v1/connectionpoints.json?limit=-1')\n",
    "conn = pd.DataFrame(jsn.json()['connectionpoints'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Herunterladen der Daten wurde eine Python Klasse geschrieben, welche die Daten für einen gebenen Bereich von der Api lädt. Diese werden anschließend sowohl in einer SQLite Datenbank, wie auch in Parquet-Dateien mithilfe von Spark gespeichert.\n",
    "\n",
    "Die Datenstruktur sieht wie folgt aus:\n",
    "\n",
    "![Datenbank-Struktur](Diagrams-ENTSO-G.png)\n",
    "\n",
    "Um auf die Daten nachfolgend zuzugreifen verwenden wir Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "conf = SparkConf().setAppName('entsog').setMaster('local')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkfolder='data/entsog_spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing parquet file\n",
    "conn.to_parquet(f'{sparkfolder}/connectionpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interconnections = spark.read.parquet(f\"{sparkfolder}/Interconnections.parquet\")\n",
    "balancingzones = spark.read.parquet(f\"{sparkfolder}/balancingzones.parquet\")\n",
    "opdata = spark.read.parquet(f\"{sparkfolder}/operationaldata\")\n",
    "oppointdirs = spark.read.parquet(f\"{sparkfolder}/operatorpointdirections.parquet\")\n",
    "operators = spark.read.parquet(f\"{sparkfolder}/operators.parquet\")\n",
    "connectionpoints = spark.read.parquet(f\"{sparkfolder}/connectionpoints.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalRemarks must be deleted as column contains different datatypes\n",
    "\n",
    "# structure files in parquet, so that hierarchy is better:\n",
    "%time opdata.drop('generalRemarks').write.partitionBy(['year','month']).parquet(f\"{sparkfolder}/operationaldata_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting time column to be grouped by a given period works as easy as this:\n",
    "from pyspark.sql.functions import date_trunc\n",
    "opdata = ( opdata.withColumn(\"day\", date_trunc('day',\"periodFrom\"))\n",
    "          .withColumn(\"hour\", date_trunc('hour',\"periodFrom\"))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only necessary stuff from static tables\n",
    "inter= interconnections.select(['pointLabel','pointTpMapX','pointTpMapY','fromCountryKey','fromOperatorLabel','fromBzLabel','fromBzLabel','toOperatorLabel','toCountryKey','pointKey','toPointKey','toPointLabel']).toPandas()\n",
    "balancing= balancingzones.select(['bzLabel','tpMapX','tpMapY']).toPandas()\n",
    "conn = (connectionpoints.withColumn(\"long\", connectionpoints.tpMapX)\n",
    "        .withColumn(\"lat\", connectionpoints.tpMapY)\n",
    "        .select(['pointKey','lat','long','infrastructureLabel']).toPandas()\n",
    "       )\n",
    "ops=operators.select(['operatorKey','operatorLabel','operatorCountryKey','operatorTypeLabelLong']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show small table of available operators\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=['OperatorKey','Betreiber Bezeichnung','Land des Betreibers','Art des Betreibers'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[ops.operatorKey, ops.operatorLabel, ops.operatorCountryKey, ops.operatorTypeLabelLong],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Die x und y Koordinaten in den Daten für Interconnections und Bidding zones sind leider nicht in Koordinaten-Form zu bringen. Keine Projektion oder Verdrehung brachte ein ordentliches Bild.\n",
    "\n",
    "Das Problem wurde anschließend dadurch gelöst, indem die Map-Tile Bilder direkt von dem ENTSO-G Server herunter geladen wurden und das Kartenmaterial nun selbst lokal gehostet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kartenmaterial beschaffung\n",
    "Die Kartendaten laden wir direkt von deren Server runter.\n",
    "Hier gibt es 5 Zoomstufen, für die jeweils Kartenmaterial als Bilddatei heruntergeladen werden kann.\\\n",
    "Außerdem gibt es verschiedene Overlays für Karten, welche angezeigt werden können.\n",
    "\n",
    "Leider sind die Daten im TMS-Format, das heißt, dass die Y-Achse invertiert ist.\n",
    "\n",
    "Für mehr Informationen, siehe hier: https://gist.github.com/tmcw/4954720\n",
    "\n",
    "Dort ist auch beschrieben, wie man diese Daten in das XYZ konvertieren kann. Das ist notwendig, da Plotly TMS leider [nicht unterstützt](https://github.com/plotly/plotly.py/issues/1610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import entsog_maploader\n",
    "\n",
    "#shutil.rmtree(\"data_xyz\")\n",
    "\n",
    "routes = ['countries_zones','pipelines_small_medium_large','pipelines_medium_large','pipelines_large','drilling_platforms','gasfields','projects','country_names']\n",
    "url = \"https://transparency.entsog.eu/assets/images/map_layers/\"\n",
    "#for route in routes:\n",
    "#    loadMap('data_tms',route,url)\n",
    "#    convertTmsXyz('data_tms/'+route,'data_xyz/'+route)\n",
    "\n",
    "#shutil.make_archive('data_xyz', 'zip', 'data_xyz')                    \n",
    "\n",
    "import inspect\n",
    "lines = inspect.getsource(entsog_maploader.loadMap)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte hier nun auch ein GeoJSON erstellen um eine Chloropleth Karte der Gebiete zu erstellen, hierzu müsste man auf https://geojson.io das Kartenmaterial als Layer einbinden und die Polygone der Marktgebiete einzeichnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben diesem Nebenschauplatz, besteht natürlich auch der Hauptteil, welcher das Herunterladen, strukturieren und abfragen der Daten beinhaltet.\n",
    "\n",
    "Die Daten können nun über die API herunter geladen werden und in einer Datenbank oder Parquet-Datei abgespeichert werden.\n",
    "\n",
    "Insgesamt haben die Daten bereits eine hohe Qualität, dennoch gibt es einige Fallstricke die man beachten muss.\n",
    "So gibt es Pipe-in-Pipe Verbindungen, welche bei Nichtbeachtung eine doppelte Buchung abgeben.\n",
    "\n",
    "Diese gilt es herauszufiltern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "routes = ['countries_zones','pipelines_small_medium_large','pipelines_medium_large','pipelines_large','drilling_platforms','gasfields','projects','country_names']\n",
    "routes = ['countries_zones','pipelines_small_medium_large','country_names']\n",
    "\n",
    "for route in routes:\n",
    "    layers.append({   \"below\": 'traces',\n",
    "                \"sourcetype\": \"raster\",\n",
    "                \"sourceattribution\": '<a href=\"https://transparency.entsog.eu/#/map\">ENTSO-G Data</a>',\n",
    "                \"source\": [\n",
    "                    \"data/mapdata_xyz/\"+route+\"/{z}/{x}/{y}.png\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karte der Verbindungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "#px.colors.qualitative.swatches()\n",
    "\n",
    "# nur die GrenzFälle\n",
    "inter_border= inter[inter['fromCountryKey'] !=inter['toCountryKey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_mapbox(inter_border, lat=\"pointTpMapY\", lon=\"pointTpMapX\", hover_name=\"pointLabel\", color='fromCountryKey',hover_data=[\"pointKey\", \"fromCountryKey\",'fromOperatorLabel','toOperatorLabel',\"toCountryKey\"],\n",
    "                        zoom=1, height=600)\n",
    "fig.update_layout(mapbox_style=\"white-bg\",mapbox_layers=layers,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karte der Marktgebiete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "fig = px.scatter_mapbox(balancing, lat=\"tpMapY\", lon=\"tpMapX\", hover_name=\"bzLabel\", #color='bzLabel',\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=1, height=600,title='Karte der Marktgebiete')\n",
    "fig.update_layout(mapbox_style=\"white-bg\",mapbox_layers=layers,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karte der Verbindungspunkte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "fig = px.scatter_mapbox(conn, lat=\"lat\", lon=\"long\", hover_name=\"pointKey\", hover_data=[\"infrastructureLabel\"],color=\"infrastructureLabel\",\n",
    "                        zoom=1, height=600,title='Karte der Verbindungspunkte')\n",
    "fig.update_layout(mapbox_style=\"white-bg\",mapbox_layers=layers,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswertungen\n",
    "\n",
    "Kommen wir nun zurück zu den Gasdatenflüssen, die wir Visualisieren möchten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg=opdata.select('periodFrom').rdd.min()\n",
    "end=opdata.select('periodFrom').rdd.max()\n",
    "\n",
    "print('Datensequenz von {} bis {}'.format(beg[0],end[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators=opdata.select('operatorKey','operatorLabel').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc\n",
    "pdf =(\n",
    "    opdata.withColumn(\"day\", date_trunc('day',\"periodFrom\"))\n",
    "    .withColumn(\"month\", date_trunc('month',\"periodFrom\"))\n",
    "    .withColumn(\"year\", date_trunc('year',\"periodFrom\"))\n",
    ").select('operatorKey','year','value').groupby(['operatorKey','year']).sum('value')\n",
    "%time pdf = pdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators.sort_values('operatorKey')\n",
    "o = (opdata.withColumn(\"day\", date_trunc('day',\"periodFrom\"))\n",
    "     .select(['value','day'])\n",
    "     .filter('operatorKey = \"DE-TSO-0013\"')\n",
    "     .groupby('day').sum('value')\n",
    "    )\n",
    "%time pdf2=  o.toPandas()\n",
    "pdf2 = pdf2.sort_values('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pdf =(opdata).select('operatorKey','year','value').groupby(['operatorKey','year']).sum('value').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.scatter(pdf['day'],pdf['sum(value)'])\n",
    "\n",
    "px.line(pdf2,x='day',y='sum(value)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.pivot(index='year', columns='operatorKey',values='sum(value)').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beanspruchung der Pipelines\n",
    "\n",
    "Auslastung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sp = opdata.select(['year','month','operatorKey','value']).groupby(['operatorKey']).sum('value').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(sp, x=\"operatorKey\", y=\"sum(value)\")\n",
    "\n",
    "fig.update_xaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verlauf der Nordstream\n",
    "\n",
    "Auslastung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Nord Stream hat als interne Bezeichnung den pointKey ITP-00120.\n",
    "Sie fördert Gas von RU nach DE.\n",
    "Genau genommen zu den PointKeys \n",
    "\n",
    "* ITP-00491 (Greifswald)\n",
    "* ITP-00250 (Greifswald)\n",
    "* ITP-00251 (Gw-Opal)? Wird hier aus der OPAL eingespeist? Unsinn?\n",
    "* ITP-00247 (Greifswald)\n",
    "* ITP-00297 (Greifswald)\n",
    "* ITP-00454 (Lubmin, reguliert)\n",
    "\n",
    "Diese Punkte erhalten Gas von der Nordstream und liegen allesamt in Deutschland.\n",
    "\n",
    "Die BZ erhält man aus den Interconnections, allerdings nur für die (tote) Leitung von RU nach DE: GASPOOL\n",
    "Die Datenlage mit Punkten außerhalb der EU ist hier sehr schlechter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter[(inter.toOperatorLabel=='Nord Stream')|(inter.fromOperatorLabel=='Nord Stream')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pips =list(inter[(inter.toOperatorLabel=='Nord Stream')|(inter.fromOperatorLabel=='Nord Stream')].toPointKey)\n",
    "pips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time d= opdata.filter(opdata.pointKey.isin(pips)).groupby(['pointKey','directionKey','day']).sum('value').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silvester = (\n",
    "    opdata#.select(['year','hour','operatorKey','value','pointKey'])\n",
    "    .filter('\"2019-01-01\" <= day and day < \"2019-02-01\"')\n",
    "    )\n",
    "silvester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time sil = silvester.filter(silvester.pointKey.isin(pips)).groupby(['pointLabel','directionKey','hour']).sum('value').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sil=sil.sort_values('hour')\n",
    "fig = px.line(sil, x='hour', y='sum(value)', color='pointLabel', title='Durchsatz der Nordstream Pipeline')\n",
    "fig.update_layout( xaxis_title='Datum',\n",
    "                   yaxis_title='Durchsatz in kW gemittelt pro Stunde')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = d.pivot(index='day', columns=['pointKey','directionKey'],values='sum(value)')\n",
    "import plotly.express as px\n",
    "\n",
    "d=d.sort_values('day')\n",
    "fig = px.line(d, x='day', y='sum(value)', color='pointKey', title='Durchsatz der Nordstream Pipeline')\n",
    "fig.update_layout( xaxis_title='Datum',\n",
    "                   yaxis_title='Durchsatz in kWh/')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.pivot(index='day', columns=['pointKey','directionKey'],values='sum(value)').plot()\n",
    "#vs melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical Flow Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "from contextlib import closing\n",
    "import pandas as pd\n",
    "from entsog_data_manager import Filter\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "ftime = {'day': '%Y-%m-%d',\n",
    "         'month': '%Y-%m-01',\n",
    "         'year': '%Y-01-01',\n",
    "         'hour': '%Y-%m-%d %H:00:00',\n",
    "         'minute': '%Y-%m-%d %H:%M:00'}\n",
    "\n",
    "def timeFilter(filt: Filter):\n",
    "    return f'\"{filt.begin.strftime(\"%Y-%m-%d\")}\" < periodFrom and periodFrom < \"{filt.end.strftime(\"%Y-%m-%d\")}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(name,filt: Filter, direction=None):\n",
    "    conn = sql.connect('data/Firm Technical.db')\n",
    "    \n",
    "    selectString = f\"strftime('{ftime[filt.groupby]}', periodFrom) as time, sum(value) as 'value'\"\n",
    "    whereString = f\" directionKey='{direction}' and adjacentSystemsLabel='{name}' and {timeFilter(filt)}\"\n",
    "    groupString = f'strftime(\"{ftime[filt.groupby]}\", \"time\")'\n",
    "    \n",
    "    if direction:\n",
    "        query = f\"SELECT {selectString} FROM AggregatedData WHERE {whereString} group by  {groupString}\"\n",
    "    else:\n",
    "        query = \"select value-exit_value as diff,a.periodFrom,value, exit_value from (SELECT periodFrom,value FROM AggregatedData WHERE directionKey='entry' and adjacentSystemsLabel='\"+name+\"') a\"\n",
    "        query += \" join (select periodFrom,value as exit_value from AggregatedData where directionKey='exit' and adjacentSystemsLabel='\"+name+\"') b on a.periodFrom = b.periodFrom\"\n",
    "        #query += \" where a.periodFrom < '2018-01-01'\"\n",
    "        #and operatorKey='BE-TSO-0001'\n",
    "    df = pd.read_sql_query(query,conn)\n",
    "    #df['begin']=pd.to_datetime(df['periodFrom']).dt.to_period('M').dt.to_timestamp()\n",
    "    #df['begin']=pd.to_datetime(df['periodFrom']).dt.floor('d')\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def update(name,filt):\n",
    "    entrys = getData(name,filt,'entry')\n",
    "    exits = getData(name,filt,'exit')\n",
    "    diff = entrys['value']-exits['value']\n",
    "    entrys['diff']= diff\n",
    "    entrys['exit']=exits['value']\n",
    "    return entrys\n",
    "\n",
    "#operatorKey\n",
    "with closing(sql.connect('data/Firm Technical.db')) as conn:\n",
    "    #operators2 = pd.read_sql_query('select distinct operatorKey,operatorLabel from AggregatedData',conn)\n",
    "    #operators = pd.read_sql_query('select distinct adjacentSystemsKey,adjacentSystemsLabel from AggregatedData',conn)\n",
    "    \n",
    "    #operators.to_sql('operators',conn)\n",
    "    #operators2.to_sql('operators2',conn)\n",
    "    operators = pd.read_sql('select * from operators', conn)\n",
    "    operators2 = pd.read_sql('select * from operators2',conn)\n",
    "    \n",
    "filt = Filter(datetime(2018, 4, 1), datetime(2018, 7, 2), 'day')\n",
    "%time data = update('NCG',filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "month = widgets.IntSlider(\n",
    "    value=1.0,\n",
    "    min=1.0,\n",
    "    max=12.0,\n",
    "    step=1.0,\n",
    "    description='Month:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "use_date = widgets.Checkbox(\n",
    "    description='Date: ',\n",
    "    value=True,\n",
    ")\n",
    "\n",
    "container = widgets.HBox(children=[use_date, month])\n",
    "\n",
    "textbox = widgets.Dropdown(\n",
    "    description='Operators:   ',\n",
    "    value=operators['operators'][0],\n",
    "    options=operators['operators'].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(x=data['time'],y=data['value'],name='importiertes Gas')#,mode='markers')\n",
    "trace2 = go.Scatter(x=data['time'],y=data['exit'],name='exportiertes Gas')#,mode='markers')\n",
    "trace3 = go.Scatter(x=data['time'],y=data['diff'],name='dem Gebiet beigefügtes Gas')#,mode='markers')\n",
    "               #hover_name=\"bzLabel\", hover_data=[\"bzTooltip\", \"id\"])    \n",
    "# Assign an empty figure widget with two traces\n",
    "#trace1 = go.Histogram(x=df['arr_delay'], opacity=0.75, name='Arrival Delays')\n",
    "#trace2 = go.Histogram(x=df['dep_delay'], opacity=0.75, name='Departure Delays')\n",
    "g = go.FigureWidget(data=[trace1, trace2,trace3],\n",
    "                    layout=go.Layout(\n",
    "                        title=dict(\n",
    "                            text='ENTSOG Physical Flow'\n",
    "                        ),\n",
    "                        xaxis=dict(title='Datum'),\n",
    "                        yaxis=dict(title='gefördertes Gas in kWh')\n",
    "                        #barmode='overlay'\n",
    "                    ))\n",
    "\n",
    "def validate():\n",
    "    if textbox.value in operators['operators'].to_list():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "import time\n",
    "def response(change):\n",
    "    if validate():\n",
    "        print('updating...')\n",
    "        t = time.time()\n",
    "        temp_df = update(textbox.value,filt=filt)\n",
    "        x1 = temp_df['value']\n",
    "        x2 = temp_df['exit']\n",
    "        x3 = temp_df['diff']\n",
    "        print('finished updating', time.time()-t)\n",
    "        with g.batch_update():\n",
    "            g.data[0].y = x1\n",
    "            g.data[1].y = x2\n",
    "            g.data[2].y = x3\n",
    "            #g.layout.xaxis.title = 'Datum'\n",
    "            #g.layout.yaxis.title = 'gefördertes Gas in kWh'\n",
    "    else:\n",
    "        print('Invalid name: ', textbox.value)\n",
    "\n",
    "textbox.observe(response, names=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.VBox([widgets.HBox([textbox]),\n",
    "              g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abweichungen des Physical Flows zu den Allokationen darstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsog_sqlite_manager import EntsogSQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entsog = EntsogSQLite('data/entsog.db')\n",
    "operators = entsog.operators()\n",
    "\n",
    "start = datetime(2018, 7, 1)\n",
    "end = datetime(2018, 7, 22)\n",
    "group = 'hour'\n",
    "filt = Filter(start, end, group)\n",
    "balzones = entsog.balancingzones()\n",
    "intercon = entsog.interconnections()\n",
    "cpp = entsog.connectionpoints()\n",
    "#gen = generation.melt(var_name='kind', value_name='value',ignore_index=False)\n",
    "operatorKeys = ['DE-TSO-0004', 'DE-TSO-0007', 'DE-TSO-0005', 'DE-TSO-0006']\n",
    "\n",
    "phy = entsog.operationaldata(operatorKeys, filt, group_by=[\n",
    "                             'operatorKey', 'directionKey'])\n",
    "piv = phy.pivot(columns=['operatorKey', 'directionKey'], values='value')\n",
    "piv.plot(rot=45, title='Physical flow of selected operators by direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = entsog.operationaldataByPoints(\n",
    "    ['ITP-00043', 'ITP-00111'], Filter(start, end, group), ['pointKey', 'directionKey'])\n",
    "point['point'] = point['pointLabel']+' ' + \\\n",
    "    point['directionKey']+' '+point['indicator']\n",
    "point['value'] = point['value']/1e6\n",
    "piv2 = point.pivot(columns=['point'], values='value')\n",
    "piv2.plot(rot=45, title='Physical flow of selected points by direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime(2018, 7, 2)\n",
    "filt = Filter(start, end, 'hour')\n",
    "operatorKeys = entsog.operatorsByBZ('Italy')\n",
    "\n",
    "operatorKeys = entsog.operatorsByBZ('Portugal')\n",
    "bil = entsog.bilanz(operatorKeys, filt)\n",
    "bil.plot(rot=45, title='netto Physical flow of Portugal seperated by distribution kind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorKeys = entsog.operatorsByBZ('GASPOOL')\n",
    "c = entsog.crossborder(operatorKeys, filt)\n",
    "c.plot(rot=45, title='Transferred physical flow of GASPOOL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flussdiagramm\n",
    "\n",
    "nach Datum über die Zeit oder als Übergangsmatrix/Sankey-Diagramm\n",
    "\n",
    "Gruppierung nach Land/Marktgebiet\n",
    "\n",
    "\n",
    "Visualisieren wo Gas benötigt wird und wo es nur geschickt wird\n",
    "\n",
    "Visualisieren der Gas-Pipeline (Kartenfehler finden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anteil Gasversorgung eines Operators am Marktgebiet\n",
    "\n",
    "Erzeugen eines DashBoards der vorhandenen Daten, ähnlich wie Energy-charts.info\n",
    "\n",
    "In einem Marktgebiet kaufen und verkaufen verschiedene Operatoren Gas.\n",
    "Pro Marktgebiet kann man jeweils für Käufe und Verkäufe die Menge nach Operator aufschlüsseln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alles weitere findet sich dann im großen Dashboard:\n",
    "\n",
    "https://nicht.datensch.eu/entsog\n",
    "\n",
    "oder aus dem FH-Aachen-VPN:\n",
    "\n",
    "https://service-fb9.fh-aachen.de/energy/entsog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
